---
title: "Going Beyond Ideal Point Points: Modeling Measurement Model Measurement Error"
author: "Bertrand Wilden"
date: "`r Sys.Date()`"
format:
  pdf:
    documentclass: article
    number-sections: true
    geometry: 
      - left=25mm
      - right=25mm
    indent: true
    fontsize: 11pt
    linestretch: 2
    fig-cap-location: top
    include-in-header:
      text:
        \usepackage{amsmath}
        \usepackage{bm}
bibliography: [references.bib, packages.bib]
nocite : |
  @R-dplyr, @R-ggdist, @R-ggplot2, @R-ggtext, @R-MetBrewer, @R-patchwork, @R-targets, @R-cmdstanr, @R-here, @R-MCMCpack, @R-purrr, @R-readr, @R-stantargets
execute: 
  echo: false
  message: false
  warning: false
---

```{r}
library(targets)
library(dplyr)
library(ggplot2)
library(ggdist)
library(ggtext)
library(patchwork)
library(MetBrewer)
library(cmdstanr)
library(MCMCpack)

knitr::write_bib(.packages(), "packages.bib")
```

```{=tex}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\tikzset{
  > = stealth,
  every node/.append style = {
    draw = none
  },
  every path/.append style = {
    arrows = ->
  },
  hidden/.style = {
    draw = black,
    shape = circle,
    inner sep = 1pt
  }
}
```

```{=tex}
\begin{center}
\begin{tikzpicture}[node distance=2cm]

  % Nodes
  \node[circle, dashed, minimum size=.5cm, draw=black, fill=white] (theta) {$\theta$};
  \node[left of=theta] (etheta) {$e_{\theta}$};
  \node[below right of=theta, circle, dashed, minimum size=.5cm, draw=black, fill=white] (e) {$E$};
  \node[right of=e] (r) {$R$};
  \node[above of=r] (v) {$V$};
  \node[below of=r] (ee) {$e_E$};
  \node[above right of=theta] (thetastar) {$\theta^*$};

  % Edges
  \draw[->] (theta) -- (thetastar);
  \draw[->] (etheta) -- (thetastar);
  \draw[->] (e) -- (r);
  \draw[->] (e) -- (theta);
  \draw[->] (thetastar) -- (v);
  \draw[->] (e) -- (v);
  \draw[->] (ee) -- (r);

\end{tikzpicture}
\end{center}


```

# Introduction

Variables of interest in the social sciences are often things we cannot directly observe or measure. Examples include the level of democracy or corruption in a country, or the political ideology of an individual or group. Latent variables such as these must be inferred through indirect processes. One common method is to build statistical models which purport to estimate latent variables using observable input data. I will refer to these as *measurement models*. The outputs of measurement models are then used in subsequent inference procedures to test substantive theories in social science. I will refer to this set of models as *theory-testing models*.

In practice, information about the latent variable is often lost when researchers move from measurement to theory-testing. Measurement models do not simply output a single value for the underlying latent variable. Instead, by virtue of being statistical models, they produce *estimates of uncertainty* for each observation. This is particularly true for Bayesian measurement models, whose output is the full posterior distribution of values according their relative plausibility---not a single point estimate and standard error as is the case for frequentist models. Failure to propagate this uncertainty from the measurement model into the theory-testing model, as I will show, can lead to mistaken conclusions regarding the underlying research question. And unlike so-called "classical" measurement error, whose attenuation bias is generally well known, the mistakes I investigate can lead to bias in unpredictable directions.

In this paper I demonstrate the problems associated with failing to include measurement model measurement error in theory-testing models, and I develop a method for overcoming these issues. By faithfully incorporating measurement uncertainty into the theory-testing stage of analysis, I show how both attenuation and confounding bias can be mitigated. While the logic of this method can be applied to any measurement model which produces estimates of uncertainty, I focus specifically on continuous-valued latent variables generated from a Bayesian measurement model. 

Theory-testing research which uses estimates from measurement models typically reduces the posterior distributions down to a single *maximum a posteriori* (MAP) value. In the case of continuous variables, researchers select some statistic of central tendency from each posterior distribution to use in subsequent analyses, such as the mean, median, or mode. This practice necessarily discards information from the full distribution. @fig-measurement-model show four hypothetical posterior distributions that may arise from a Bayesian measurement model. Despite all having the same mean of zero, higher order moments such as variance (top-right), skew (bottom-left), and kurtosis (bottom-right) can create distributions which vary to a large extent.

```{r}
#| label: fig-measurement-model
#| fig-cap: "Ignoring Measurement Error in Measurement Models"
empty_labeller <- function(variable, value) {
  return("")
}

N <- 100000
tibble(x = c(rnorm(N, sd = 1),
             rnorm(N, sd = 2),
             sn::rsn(N, alpha = -4, omega = 2, xi = 1.5),
             sn::rst(N, xi = -2.75, omega = 2, alpha = 4, nu = 1)),
       y = c(rep("A\nNormal(0, 1)", N), 
             rep("B\nNormal(0, 2)", N), 
             rep("C\nSkew Normal(1.5, 2, -4)", N), 
             rep("D\nSkew t(-2.75, 2, 4, 1)", N))) |> 
  ggplot(aes(x = x)) +
  geom_histogram(color = "white", fill = "darkcyan", alpha = .5, bins = 50) +
  geom_vline(xintercept = 0, linetype = "twodash", color = "black", linewidth = .7) +
  geom_text(aes(label = y), x = -4, y = 0) +
  facet_wrap(~ y, nrow = 2) +
  xlim(-5, 5) +
  theme_ggdist() +
  labs(y = "", x = "", caption = "Four different measurement model posterior distributions with mean zero") +
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(family = "serif"))
```

In @fig-measurement-model, an estimate from distribution B should be treated as more uncertain than one from distribution A when used to test a theory. Failing to do so, as I will show, can lead to attenuation bias---or the false conclusion that the latent variable has no association with an outcome when it in fact does. In other words, the method I propose can help increase the statistical power of theory tests. Panels C and D in #fig-measurement-model show skewed distributions. Here the danger is that the skewness is caused by a third variable, which also causes the outcome of interest in the theory-testing model. This, as I will show, can lead to confounding bias if the skewness of the measurement output is not accounted for.

## Method Overview

How can researchers avoid the issues highlighted above? In short, the measurement process and theory-testing procedure should happen simultaneously in a single model. This is handled straightforwardly using the Bayesian statistical framework, which can easily treat parameters and data interchangeably [@mcelreath2020]. We start by specifying the full measurement model, whose posterior distributions for each observation's value of the latent variable are then used as data in the theory-testing model. The stylized version of this joint model is shown in @eq-ex-equation, where $g(\cdot)$ is the measurement model which produces posterior estimates of the latent variable, $\theta_i$ for each observation based on some training data $y^*$. The posterior estimates for $\theta_i$ from the measurement model $g(\cdot)$ are then used as data in the theory-testing model $f(\theta_i)$ using the outcome of interest $y$.

$$
\begin{aligned}
  y_i &\sim f(\theta_i) \\
  y^*_i &\sim g(\theta_i)
\end{aligned}
$$ {#eq-ex-equation}

There are two practical issues, however, with building a fully-specified joint measurement and theory-testing model. The first is computational. Bayesian statistical software uses notoriously expensive Markov Chain Monte Carlo (MCMC) sampling methods to derive its posterior distributions. Even on their own, measurement models which use MCMC can be extremely slow to sample given these types of models' high-dimensional nature. So attempting to sample from a model which also includes an arbitrarily complex theory-testing model, $f(\cdot)$, in addition to the measurement model may simply be unfeasible given the computing power that the average researcher has access to. The second challenge with the idealized joint model is that it requires researchers to write down a fully-specified measurement model, $g(\cdot)$. Compared to their theory-testing model, applied researchers likely have much less knowledge regarding the intricacies involved in estimating latent variables. Measurement models can often be challenging to fit in practice due to issues of identifiability.

My method overcomes the two problems outlined above by simplifying the measurement model step, $y^*_i \sim g(\cdot)$ in the joint model. Rather than estimating the latent variable from scratch, I take the posterior distributions already provided from previously fitted measurement models and use those as approximations in the full joint model. The measurement model $g(\cdot)$ becomes a probability distribution function with distributional parameters according to maximum likelihood estimates of the posterior. So if the posterior distribution of the measurement model appears normal, we would use $\mathbb{E}[\theta_i] \sim N(\theta_i, \sigma_i)$. The values $\mathbb{E}[\theta_i]$ and $\sigma_i$ are estimated from the measurement model's posterior distribution, which allows the true, unobserved, value of the latent variable $\theta_i$ to be estimated for each observation. If the posterior distributions from the measurement model appear skewed, or have thicker tails than a normal distribution, the distributional parameters for these distributions can be used instead. These simplifications faithfully propagate the uncertainty in the outputs of the measurement model to the theory-testing model, while also being computationally tractable and straightforward to implement.

## Motivating Example - Bayesian Models of Ideology

One of the most common measurement models in political science is the Bayesian Item-Response Theory (IRT) model used to measure the ideological leanings of political actors [@clinton2004; @bafumi2005]. These models assume that political ideology is a latent characteristic that lies on a single left-right dimension. Observed actions, such as voting on legislation, are used as training data ($y^*$ in @eq-ex-equation) to produce a posterior distribution of continuous values for each actor (e.g. member of Congress) on this left-right scale. [*insert footnote about how these models can measure other types of latent variables*]

Let's say we wanted to estimate the effect of legislator ideology, $\theta$, on some outcome, $y$. Using the format I developed in @eq-ex-equation, @eq-irt shows an example joint measurement and theory-testing model to answer this question using ideology estimates from an IRT model. The measurement model (bottom) predicts whether a legislator voted yes or no on a piece of legislation, $y^*_j$, using the traditional 2-parameter IRT equation $\Phi(\gamma_j\theta_i + \xi_j)$. Estimates of the parameters $\theta_i$ from this model, are then used as data in the linear regression theory-testing model (top) to estimate $\beta$---the coefficient of theoretical interest.

$$
\begin{aligned}
  y_i &\sim \text{Normal}(\alpha + \theta_i \beta, \sigma^2) \\
  y^*_{ij} &\sim \text{Bernoulli}[\Phi(\gamma_j\theta_i + \eta_i)] \\
\end{aligned}
$$ {#eq-irt}

As mentioned previously, however, estimating @eq-irt would not generally be feasible due to computational constraints. Instead, the IRT ideology measurement model should be fit beforehand. Then, for each legislator's posterior distribution of $\theta$, the values $\mathbb{E}[\theta_i]$ and $\sigma^2_{\theta i}$ should be calculated. These values can then used as data in the simplified measurement model in @eq-irt-me in order to estimate the latent $\theta_i$ for each observation.

$$
\begin{aligned}
  y_i &\sim \text{Normal}(\alpha + \theta_i \beta, \sigma^2) \\
  \mathbb{E}[\theta_i] &\sim \text{Normal}(\theta_i, \sigma^2_{\theta i})
\end{aligned}
$$ {#eq-irt-me}

If the posterior estimates of $\theta$ from the IRT measurement model are truly normally distributed for each legislator, then @eq-irt and @eq-irt-me are essentially equivalent---thereby properly incorporating the measurement model measurement error in the theory-testing model. If, however, the IRT model produces posterior distributions that are not normal, then this simplification step could be throwing out important information. For this reason I extend the model to include a skewness parameter later in this project.

# Measurement Error Models

In this section I will provide additional motivation for why researchers should care about measurement model uncertainty when using latent variables in their theory-testing models. Usually, theory-testing models are used to answer some causal question: *what is the effect of X on Y*? The observed relationship between X and Y is often confounded by other variables in the system exerting causal influence. Theory-testing models, therefore, need to condition on these confounding variables in order to get an unbiased estimate of the causal effect of interest. While this general method for theory testing is well-understood in the social sciences [@rubin1974; @morgan2007], it is less common to apply the same causal logic to measurement. Failing to do so, I argue, can lead to erroneous substantive conclusions.

I will demonstrate my argument using the causal graph framework [@pearl2000]. Causal graphs are heuristic tools that map out causal relationships between variables in a particular system. Each node represents a variable, and the directed edges between nodes represent hypothesized causal impacts of one variable on another. These directed-acyclic-graphs (DAGs) are useful because they allow us to determine the set of variables we need to condition on in order to get an unbiased estimate of the effect of our primary independent variable on the dependent variable. This set of confounders is defined by the variables which are needed to close every "backdoor" path between the primary independent and dependent variables.[^1]

[^1]: See @cinelli2020 for a more complete introduction to deconfounding using DAGs.

## Measurement Error Confounding Bias

@fig-irt-dag shows a simple DAG outlining the causal process implied by the IRT ideology measurement model. The estimated ideology variable produced by the model, $\theta^*$ is a function of an individual's true ideology, $\theta$ and measurement error, $e^{\theta}$. The important takeaway from @fig-irt-dag is that the ideology variable produced by IRT models arises causally from a combination of some latent true ideology (which we do not observe), and from measurement error (which we observe, at least partially, in the form of the posterior distribution for $\theta^*$). There is likely also considerable unobserved measurement error in these models that can be due to a variety of factors. IRT models assume each legislator's decision to vote on bill is influenced solely by their innate ideology, rather than on strategic concerns. A violation of this assumption, therefore, will produce biased estimates of $\theta^*$. There are also computational issues with fitting IRT models which may make the posterior estimates untrustworthy. For the purposes of this illustration, however, I will assume that the posterior distribution, $e^{\theta}$ for $\theta^*$ contains all relevant information about the measurement error for the true ideology $\theta$.

::: {#fig-irt-dag fig-cap="IRT Ideology Measurement Error Model"}
```{=tex}
\begin{Large}
\begin{center}
\tikz{
    \node (race_s) {$R^*$};
    \node (race_s_label) [right = .1 of race_s] {$\textcolor{teal}{\text{\normalsize Estimated Race}}$};
    \node[hidden, dashed] (race) [above = 1.5 of race_s] {$R$};
    \node (race_label) [above = .1 of race] {$\textcolor{teal}{\text{\normalsize True Race}}$};
    \node (e_race) [left = 1.5 of race_s] {$e^R$};
    \node (e_race_label) [left = 0 of e_race] {$\textcolor{teal}{\text{\normalsize Measurement Error}}$};   
    
    \path[dashed] (race) edge (race_s);
    \path (e_race) edge (race_s);
}
\end{center}
\end{Large}
```
:::

Now consider a hypothetical theory-testing model constructed to determine whether there were racial disparities in voter turnout, $Y$ after the enactment of voter identification laws at time, $T$. Suppose we do not have data on individuals' self-reported race, and therefore must use BSIG to estimate $R^*$. @fig-race-dag-theory shows a potential causal graph for this system. The main causal effect of interest is represented by the path $R^* \longrightarrow Y$, moderated by $T$. In order to get an unbiased estimate of this causal effect we need to close all backdoor paths leading from $R^*$ to $Y$, which in this case, flows through the unobserved variable $U$. This confound represents anything that is a common cause of both the BSIG measurement error and voter turnout. @argyle2022 systematically review BSIG misclassification rates and find that socio-economic status and geography affect the amount of measurement error in the algorithm. It is also highly plausible that these variables have an independent effect on turnout rates in the population.

::: {#fig-race-dag-theory fig-cap="BSIG Measurement Error in a Hypothetical Theory-Testing Model"}
```{=tex}
\begin{Large}
\begin{center}
\tikz{
    \node (race_s) {$R^*$};
    \node[hidden, dashed] (race) [above = 1.5 of race_s] {$R$};
    \node (e_race) [left = 1.5 of race_s] {$e^R$};
    \node[hidden, dashed] (confound) [below = 1.5 of race_s] {$U$};
    \node (confound_label) [below = .1 of confound] {$\textcolor{teal}{\text{\normalsize Confound}}$};  
    \node (y) [right = 1.5 of race_s] {$Y$};
    \node (y_label) [above = .1 of y] {$\textcolor{teal}{\text{\normalsize Turnout}}$};
    \node (t) [below right = 1.5 of y] {$T$};
    \node (t_label) [below = 0 of t] {$\textcolor{teal}{\text{\normalsize Time}}$};
    
    \path[dashed] (race) edge (race_s);
    \path (e_race) edge (race_s);
    \path[dashed] (confound) edge [bend left=20] (e_race);
    \path[dashed] (confound) edge [bend right=20] (y);
    \path (race_s) edge (y);
    \path (t) edge (y);
}
\end{center}
\end{Large}
```
:::

For the theory-testing model in @fig-race-dag-theory it may be possible to condition on some variables in $U$ in order to obtain an unbiased estimate of $R^* \longrightarrow Y$. But with something as multi-faceted as socio-economic status, there is always the risk of residual confounding. My proposed method of building a joint measurement and theory-testing model fixes this issue by obviating the need to deal with $U$ at all. This is because the measurement error, $e^R$ in @fig-race-dag-theory is part of the backdoor path from $R^*$ to $Y$. Therefore when we explicitly incorporate $e^R$ into a model estimating $R^* \longrightarrow Y$ we can obtain an unbiased estimate of the causal effect of race on turnout.

While I have only presented a discussion of a single theory-testing model---the impact of race on turnout---incorporating $e^R$ into any model using BSIG race estimates will be valuable. Race is often thought to be an "un-caused cause" because it is assigned from an individual's birth, hence unbeholden to influence from other variables. [@sen2016]. This removes the need to worry about potential confounds because there cannot be any backdoor paths through the variable race. But as @fig-race-dag-theory shows, BSIG *estimates* of race, $R^*$ always have the potential to have open backdoor paths through measurement error $e^R$. Simply including $e^R$ directly in a joint measurement theory-testing model blocks this backdoor path going into $R^*$, which helps identify the direct effect of race on the outcome of interest.

## Measurement Error Attenuation Bias

The previous discussion of BSIG race estimates highlighted how failing to account for measurement error could lead to bias via backdoor confounding. This general issue is known as nonrandom, or unignorable, measurement error [@blalock1970]. In the political science methodology literature, methods such as multiple imputation [@blackwell2017] and sensitivity analysis [@gallop2019; @imai2010] have been developed to deal with nonrandom measurement error. My method is another way of dealing with nonrandom measurement error, but in the context where the measurement error is known and comes from the output of some measurement model.

Measurement model errors can also take the form of classical measurement error. This is where we do not assume that there is some relationship between the measurement error and the outcome of interest, rather, the errors are simply random "noise" in the measurement estimates. Classical measurement error leads to attenuation bias: a reduction of the main effect size in the theory-testing model towards zero. As I demonstrate via simulations, my method can help correct this kind of bias in theory-testing models as well.

# Simulation Study - Bayesian IRT

::: {#fig-ideal-dag fig-cap="Bayesian IRT Measurement Model"}
```{=tex}
\begin{Large}
\begin{center}
\tikz{
    \node (theta_s) {$\theta^*$};
    \node (theta_label) [right = 0 of theta_s] {$\textcolor{teal}{\text{\normalsize Estimated Ideal Point}}$};
    \node[hidden, dashed] (theta) [above = 1.5 of theta_s] {$\theta$};
    \node (theta_label) [above = .1 of theta] {$\textcolor{teal}{\text{\normalsize True Ideal Point}}$};
    \node (e_theta) [left = 1.5 of theta_s] {$e^\theta$};
    \node (e_theta_label) [left = 0 of e_theta] {$\textcolor{teal}{\text{\normalsize Measurement Error}}$};
    \node (p) [below = 1.5 of e_theta] {$P$};
    \node (p_label) [below = 0 of p] {$\textcolor{teal}{\text{\normalsize Participation}}$};
    
    \path (e_theta) edge (theta_s);
    \path[dashed] (theta) edge (theta_s);
    \path (p) edge [bend left=0] (e_theta);
    \path[dashed] (theta) edge [bend right=30] (e_theta);
}
\end{center}
\end{Large}
```
:::

In this section I will demonstrate, through simulations, how my method of constructing a joint measurement theory-testing model can help mitigate attenuation bias that arises from classic measurement error. @fig-ideal-dag shows the causal process which produces ideal point estimates, $\theta^*$ in a Bayesian IRT model. As in the BSIG case, our observed measurements come from an unobserved latent variable plus some measurement error. At least two variables can affect the measurement error, $e^{\theta}$ in @fig-ideal-dag. The true ideal point of the group, $\theta$ influences the amount of measurement error for estimates of the group's ideology because, as we move further from the ideological center of the scale, uncertainty increases. @fig-irt-dist shows an example of what the distribution of posterior estimates from a Bayesian IRT model look like. Groups further from the center have much wider ideal point posterior distributions. The second variable affecting $e^{\theta}$ is group participation, $P$---how often the group signals its position among the items in the model. Groups that signal more positions on items will have smaller levels of measurement error compared to groups that signal fewer positions because we have more data on their true ideological preferences.

```{r}
#| label: fig-irt-dist
#| fig-cap: "Bayesian IRT Posterior Distributions"

source(here::here("R", "sim_funcs.R"))

set.seed(100)

sim_cont_data(500, noise_a = 2, true_b = 1)$dat |> 
  arrange(x_meas) |> 
  mutate(id = row_number()) |> 
  ggplot(aes(x = x_meas, y = id)) +
  geom_pointrange(aes(xmin = x_meas - x_sd, xmax = x_meas + x_sd), 
                  alpha = .35, size = .25, color = met.brewer("Isfahan1", 1)) +
  geom_pointrange(aes(xmin = x_meas - x_sd * 1.96, xmax = x_meas + x_sd * 1.96), 
                  alpha = .1, size = .25, color = met.brewer("Isfahan1", 1)) +
  # geom_point(color = "red", size = .3, alpha = .2) + 
  labs(x = expression(paste("Estimated Ideal Point, ", theta^{`*`})), y = "Groups (sorted)",
       caption = "Means and Standard Errors") +
  theme_ggdist() +
  theme(text = element_text(family = "serif"))
```

::: {#fig-ideal-dag-theory fig-cap="Bayesian IRT Measurement Model in Hypothetical Theory-Testing Model"}
```{=tex}
\begin{Large}
\begin{center}
\tikz{
    \node (theta_s) {$\theta^*$};
    \node[hidden, dashed] (theta) [above = 1.5 of theta_s] {$\theta$};
    \node (e_theta) [left = 1.5 of theta_s] {$e^\theta$};
    \node (p) [below = 1.5 of e_theta] {$P$};
    \node (y) [right = 1.5 of theta_s] {$Y$};
    \node (y_label) [above = 0 of y] {$\textcolor{teal}{\text{\normalsize Outcome}}$};
    
    \path (e_theta) edge (theta_s);
    \path (theta_s) edge (y);
    \path[dashed] (theta) edge (theta_s);
    \path (p) edge [bend left=0] (e_theta);
    \path[dashed] (theta) edge [bend right=30] (e_theta);
}
\end{center}
\end{Large}
```
:::

While there may exist some backdoor paths through $e^{\theta}$ and $P$ in a hypothetical theory-testing model, I will assume that the outcome of interest, $Y$ is unaffected by anything other than the direct causal effect $\theta^* \longrightarrow Y$.[^2] The purpose of this simplification is to highlight the consequences of random measurement error during parameter estimation of a theory-testing model. Using the generative causal model in @fig-ideal-dag-theory we can simulate data with a known parameter for the effect of group ideal point on the outcome $Y$. Then we fit two linear regression models to estimate this parameter, $\beta$. @eq-cont-no-me-model is the naive theory-testing model where $X_{\text{MEAS},i}$ corresponds to a group's mean ideal point estimate from the Bayesian IRT measurement model (see right panel of @fig-measurement-model). This is in contrast to the joint measurement theory-testing model in @eq-cont-me-model, which models $X_{\text{MEAS},i}$ as an outcome of $X_{\text{TRUE},i}$ (an unobserved parameter for each observation)[^3] and $X_{\text{SE},i}$ which is the observed standard deviation of the posterior distribution for each groups' ideal point. The estimates of $X_{\text{TRUE},i}$ are then used in the linear model which predicts the outcome $y_i$.

[^2]: In principle, we could close any backdoor paths through $P$ by conditioning on it directly because the level of group participation is directly observed.

[^3]: The parameters $X_{\text{TRUE},i}$ should not be confused with the true ideal points produced in the simulation. These true values are never seen by either model.

$$
\begin{aligned}
  y_i &\sim \text{Normal}(\mu_i, \sigma) \\[-10pt]
  \mu_i &= \alpha + \beta X_{\text{MEAS},i} \\[-10pt]
  \alpha &\sim \text{Normal}(0, 2) \\[-10pt]
  \beta &\sim \text{Normal}(0, 2) \\[-10pt]
  \sigma &\sim \text{Half Student t}(3, 0, 2) \\
\end{aligned}
$$ {#eq-cont-no-me-model}

$$
\begin{aligned}
  y_i &\sim \text{Normal}(\mu_i, \sigma) \\[-10pt]
  \mu_i &= \alpha + \beta X_{\text{TRUE},i} \\[-10pt]
  X_{\text{MEAS},i} &\sim \text{Normal}(X_{\text{TRUE},i}, X_{\text{SE},i}) \\[-10pt]
  X_{\text{TRUE}, i} &\sim \text{Normal}(0, \tau) \\[-10pt]
  \alpha &\sim \text{Normal}(0, 2) \\[-10pt]
  \beta &\sim \text{Normal}(0, 2) \\[-10pt]
  \sigma &\sim \text{Half Student t}(3, 0, 2) \\[-10pt]
  \tau &\sim \text{Half Student t}(3, 0, 2) \\
\end{aligned}
$$ {#eq-cont-me-model}

@fig-irt-sim-comparison shows how well each model recovers the true parameter value for $\beta$: the effect of the true ideal point on the simulated outcome. Each model was fit 40 times across a range of increasing random error levels (shown on the horizontal axis as the correlation between the simulated true ideal point and mean measurement error value approach zero). The mean posterior estimates of each model's $\beta$ parameter were then plotted using a loess fit. With little-to-no measurement error (left side of the graph), both models reliably recover the true $\beta$ value of 1. But as the random measurement error increases, the $\beta$ estimates from the naive model from @eq-cont-no-me-model rapidly attenuate towards zero. This is in contrast to the estimates from the joint measurement theory-testing model in @eq-cont-me-model which attenuate slightly but remain much closer to the true $\beta$ value even after there is essentially zero correlation between the true ideal points and means from the ideal points with measurement error.

```{r}
#| label: fig-irt-sim-comparison
#| fig-cap: "Parameter Recovery as Measurement Error Increases"
#| fig-height: 4
tar_load(cont_coef_plot)
cont_coef_plot
```

# Skew-Normal Distribution

```{r}
tar_load(bias_coef_plot)
bias_coef_plot
```


# Case Study: Does Political Extremism Affect Election Outcomes?

```{r}
tar_load(reelection_coef_plot)
reelection_coef_plot
```


## Extension Skew-t

## Importance of gibbs vs HMC vs no uncertainty


\newpage

# References
